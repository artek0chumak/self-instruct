{
    "output_dir": "gpt-j-6b",
    "logging_steps": 10,
    "report_to": "wandb",
    "fp16": true,
    "per_device_train_batch_size": 2,
    "gradient_accumulation_steps": 16,
    "gradient_checkpointing": true,
    "deepspeed": {
        "zero_optimization": {
            "stage": 2
        },
        "fp16": {
            "enabled": "auto",
            "loss_scale": 0,
            "loss_scale_window": 1000,
            "initial_scale_power": 10,
            "hysteresis": 2,
            "min_loss_scale": 1
        },
        "gradient_accumulation_steps": "auto",
        "train_micro_batch_size_per_gpu": "auto",
        "zero_allow_untested_optimizer": true
    }
}